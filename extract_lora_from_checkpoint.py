"""
DeepSpeed ì²´í¬í¬ì¸íŠ¸(global_step*)ì—ì„œ LoRA safetensors ì¶”ì¶œ

DeepSpeedëŠ” exclude_frozen_parameters=Trueë¡œ ì €ì¥í•˜ë¯€ë¡œ
layer_*-model_states.pt íŒŒì¼ë“¤ì— LoRA íŒŒë¼ë¯¸í„°ë§Œ ë¶„ì‚° ì €ì¥ë¨.
ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì „ë¶€ í•©ì³ì„œ ComfyUI í˜¸í™˜ safetensorsë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
  python extract_lora_from_checkpoint.py <global_step_í´ë”_ê²½ë¡œ> [ì¶œë ¥ê²½ë¡œ.safetensors]

ì˜ˆì‹œ:
  python extract_lora_from_checkpoint.py ./global_step547
  python extract_lora_from_checkpoint.py ./global_step547 my_lora.safetensors
  python extract_lora_from_checkpoint.py --batch ./20260206_04-38-55

Colabì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë²•:
  í›ˆë ¨ ì‹¤í–‰ ì¤‘ì—ë„ ì™¼ìª½ ğŸ“ íŒŒì¼ë¸Œë¼ìš°ì €ì—ì„œ global_step* í´ë”ë¥¼
  í†µì§¸ë¡œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥ (ì…€ ì‹¤í–‰ ë¶ˆí•„ìš”)

GPU ë¶ˆí•„ìš” â€” CPU + RAMë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import sys
import os
import re
import torch
from pathlib import Path

try:
    from safetensors.torch import save_file
except ImportError:
    print("âŒ safetensorsê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    print("   pip install safetensors")
    sys.exit(1)


def extract_lora_from_checkpoint(checkpoint_dir: str, output_path: str = None, save_dtype: str = "bfloat16"):
    checkpoint_dir = Path(checkpoint_dir)

    if not checkpoint_dir.exists():
        print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_dir}")
        sys.exit(1)

    # â”€â”€ 1. layer íŒŒì¼ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # DeepSpeed pipeline ì²´í¬í¬ì¸íŠ¸: layer_*-model_states.pt (exclude_frozen=True â†’ LoRAë§Œ)
    layer_files = sorted(
        checkpoint_dir.glob("layer_*-model_states.pt"),
        key=lambda p: int(re.search(r"layer_(\d+)", p.name).group(1))
    )
    # ë‹¨ì¼ íŒŒì¼ ë°©ì‹ (pipeline_stages=1 ë“±)
    mp_file = checkpoint_dir / "mp_rank_00_model_states.pt"

    if not layer_files and not mp_file.exists():
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   í´ë” ë‚´ìš©: {[f.name for f in checkpoint_dir.iterdir()]}")
        sys.exit(1)

    # ì¶œë ¥ ê²½ë¡œ ê¸°ë³¸ê°’
    if output_path is None:
        step_name = checkpoint_dir.name  # e.g., "global_step547"
        output_path = checkpoint_dir.parent / f"lora_{step_name}.safetensors"
    output_path = Path(output_path)

    print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸: {checkpoint_dir}")
    print(f"ğŸ“¦ ì¶œë ¥ ê²½ë¡œ:  {output_path}")

    # â”€â”€ 2. íŒŒë¼ë¯¸í„° ë¡œë“œ (CPU only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Anima to_layers() êµ¬ì¡°:
    #   layer 0: InitialLayer (pos_embedder, x_embedder, t_embedder ë“±)
    #   layer 1: LLMAdapterLayer (llm_adapter)
    #   layer 2 ~ N+1: TransformerLayer(blocks[0] ~ blocks[N-1])
    #   layer N+2: FinalLayer (final_layer)
    #
    # DeepSpeedëŠ” ê° layerì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë¡œì»¬ ì´ë¦„ìœ¼ë¡œ ì €ì¥í•˜ë¯€ë¡œ
    # layer ë²ˆí˜¸ì—ì„œ ì›ë˜ transformer ê²½ë¡œë¥¼ ë³µì›í•´ì•¼ í•©ë‹ˆë‹¤.

    dtype_map = {
        "bfloat16": torch.bfloat16,
        "float16": torch.float16,
        "float32": torch.float32,
    }
    target_dtype = dtype_map.get(save_dtype, torch.bfloat16)
    comfyui_sd = {}
    total_keys = 0

    if layer_files:
        print(f"â³ layer íŒŒì¼ {len(layer_files)}ê°œ ë¡œë”© ì¤‘... (CPU only)")
        for lf in layer_files:
            layer_num = int(re.search(r"layer_(\d+)", lf.name).group(1))
            layer_sd = torch.load(lf, map_location="cpu", weights_only=False)

            if len(layer_sd) == 0:
                continue

            total_keys += len(layer_sd)

            for key, value in layer_sd.items():
                # ì›ë˜ transformer ê²½ë¡œ ë³µì›
                if layer_num == 0:
                    # InitialLayer: pos_embedder.*, x_embedder.*, t_embedder.* ë“±
                    # í‚¤ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì´ë¯¸ ì˜¬ë°”ë¥¸ ì´ë¦„)
                    original_key = key
                elif layer_num == 1:
                    # LLMAdapterLayer: llm_adapter.*
                    original_key = key  # ì´ë¯¸ llm_adapter.* í˜•íƒœ
                elif layer_num >= 2 and layer_num <= len(layer_files) - 1:
                    # TransformerLayer: layer_num â†’ blocks[layer_num - 2]
                    block_idx = layer_num - 2
                    # ë¡œì»¬ ì´ë¦„: "block.self_attn.q_proj.lora_A.default.weight"
                    # ì›ë³¸ ì´ë¦„: "blocks.{block_idx}.self_attn.q_proj.lora_A.default.weight"
                    if key.startswith("block."):
                        original_key = f"blocks.{block_idx}." + key[len("block."):]
                    else:
                        original_key = f"blocks.{block_idx}.{key}"
                else:
                    # FinalLayer: final_layer.*
                    original_key = key

                # .default. ì œê±° (PEFT LoRA adapter suffix)
                clean_key = original_key.replace(".default.", ".")
                # .modules_to_save. ì œê±°
                clean_key = clean_key.replace(".modules_to_save.", ".")
                # ComfyUI í¬ë§·: diffusion_model. prefix
                if not clean_key.startswith("diffusion_model."):
                    clean_key = "diffusion_model." + clean_key

                comfyui_sd[clean_key] = value.to(target_dtype)

            lora_count = len(layer_sd)
            if lora_count > 0:
                print(f"   layer_{layer_num:02d}: {lora_count} params", end="")
                if layer_num == 0:
                    print(" (InitialLayer)")
                elif layer_num == 1:
                    print(" (LLMAdapter)")
                elif layer_num <= len(layer_files) - 1:
                    print(f" â†’ blocks.{layer_num - 2}")
                else:
                    print(" (FinalLayer)")
    else:
        # ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ (mp_rank_00)
        print(f"â³ {mp_file.name} ë¡œë”© ì¤‘... (CPU only)")
        checkpoint = torch.load(mp_file, map_location="cpu", weights_only=False)
        state_dict = checkpoint.get("module", checkpoint)
        total_keys = len(state_dict)

        for key, value in state_dict.items():
            clean_key = key.replace(".default.", ".").replace(".modules_to_save.", ".")
            if not clean_key.startswith("diffusion_model."):
                clean_key = "diffusion_model." + clean_key
            comfyui_sd[clean_key] = value.to(target_dtype)

    print(f"\n   ì „ì²´ ë¡œë“œ í‚¤: {total_keys}")
    print(f"   ì¶œë ¥ í‚¤ ìˆ˜:  {len(comfyui_sd)}")

    if len(comfyui_sd) == 0:
        print("âŒ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # í‚¤ ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°
    sample = list(comfyui_sd.keys())[:5]
    print(f"   í‚¤ ìƒ˜í”Œ: {sample}")

    # â”€â”€ 3. safetensors ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\nğŸ’¾ ì €ì¥ ì¤‘... ({save_dtype}, {len(comfyui_sd)} tensors)")
    os.makedirs(output_path.parent, exist_ok=True)
    save_file(comfyui_sd, str(output_path), metadata={"format": "pt"})

    file_size = output_path.stat().st_size / 1024 / 1024
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path} ({file_size:.1f} MB)")

    # í‚¤ ìƒ˜í”Œ ì¶œë ¥
    sample = list(comfyui_sd.keys())[:5]
    print(f"\nğŸ”‘ í‚¤ ìƒ˜í”Œ:")
    for k in sample:
        print(f"   {k} â†’ {comfyui_sd[k].shape} ({comfyui_sd[k].dtype})")

    return output_path


def batch_extract(run_dir: str, save_dtype: str = "bfloat16"):
    """run í´ë” ì•ˆì˜ ëª¨ë“  global_step*ì—ì„œ LoRA ì¶”ì¶œ"""
    run_dir = Path(run_dir)
    step_dirs = sorted(
        [d for d in run_dir.glob("global_step*") if d.is_dir()],
        key=lambda p: int(re.search(r"(\d+)$", p.name).group(1))
    )

    if not step_dirs:
        print(f"âŒ global_step* í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {run_dir}")
        sys.exit(1)

    print(f"ğŸ“‚ Run í´ë”: {run_dir}")
    print(f"ğŸ”„ {len(step_dirs)}ê°œ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬\n")

    for step_dir in step_dirs:
        print(f"{'='*60}")
        try:
            extract_lora_from_checkpoint(str(step_dir), save_dtype=save_dtype)
        except Exception as e:
            print(f"âš ï¸ {step_dir.name} ì‹¤íŒ¨: {e}")
        print()


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(__doc__)
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“œ:")
        print("  1. ë‹¨ì¼:  python extract_lora_from_checkpoint.py ./global_step547")
        print("  2. ì¼ê´„:  python extract_lora_from_checkpoint.py --batch ./run_í´ë”")
        print("  3. ì§€ì •:  python extract_lora_from_checkpoint.py ./global_step547 output.safetensors")
        sys.exit(1)

    if sys.argv[1] == "--batch":
        if len(sys.argv) < 3:
            print("âŒ run í´ë” ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.")
            sys.exit(1)
        batch_extract(sys.argv[2])
    else:
        checkpoint_path = sys.argv[1]
        output = sys.argv[2] if len(sys.argv) > 2 else None
        extract_lora_from_checkpoint(checkpoint_path, output)
